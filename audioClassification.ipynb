{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio classification\n",
    "\n",
    "classification of ***Capuchinbird*** chirps from other forest sounds and identifying the chirps in Forest recordings\n",
    "\n",
    ">dataset downloaded from kaggle\n",
    "\n",
    "***dataset contains***\n",
    "- *forest recordings*\n",
    "- *parsed Capuchinbird sounds*\n",
    "- *parsed non Capuchinbird sounds*\n",
    "\n",
    ">due to no proper support of latest version numpy with latest torch -lists are used as alternative at some spots\n",
    "\n",
    ">due to oserror torchaudio is not used instead scipy.signal module is used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from scipy.signal import spectrogram,resample\n",
    "import os\n",
    "import random as r\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWav(path,freq=16e3):\n",
    "    rate,wav=wavfile.read(path)\n",
    "    if wav.ndim>1:\n",
    "        wav=wav[:,0]\n",
    "    nSamples=int((len(wav)/rate)*freq)\n",
    "    nwav=resample(wav,nSamples)[:48000]\n",
    "    padding=48000-nwav.shape[0]\n",
    "    if padding:\n",
    "        x=padding//2\n",
    "        nwav=np.pad(nwav,(x,padding-x),'edge')\n",
    "    return nwav\n",
    "\n",
    "def getSpectrogram(wav):\n",
    "    f,t,Sxx=spectrogram(wav)\n",
    "    Sxx=np.expand_dims(Sxx,axis=0)\n",
    "    mean=Sxx.mean()\n",
    "    std=Sxx.std()\n",
    "    tensor=torch.tensor(list(Sxx),dtype=torch.float32)\n",
    "    tensor=F.normalize(tensor,mean=mean,std=std)\n",
    "    return tensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "posPATH='./sounds/Parsed_Capuchinbird_Clips/'\n",
    "negPATH='./sounds/Parsed_Not_Capuchinbird_Clips/'\n",
    "posFiles=[posPATH+i for i in os.listdir(posPATH)]\n",
    "r.shuffle(posFiles)\n",
    "negFiles=[negPATH+i for i in os.listdir(negPATH)]\n",
    "r.shuffle(negFiles)\n",
    "posData=list(zip(map(getSpectrogram,map(loadWav,posFiles[:150])),torch.ones((150,1))))\n",
    "negData=list(zip(map(getSpectrogram,map(loadWav,negFiles[:150])),torch.zeros((150,1))))\n",
    "DATA=posData+negData\n",
    "r.shuffle(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MODEL,self).__init__()\n",
    "        self.conv1=nn.Conv2d(1,16,3)\n",
    "        self.conv2=nn.Conv2d(16,16,3)\n",
    "        self.lin=nn.Linear(16*125*210,128)\n",
    "        self.lin2=nn.Linear(128,1)\n",
    "    def forward(self,x):\n",
    "        x=nn.functional.relu(self.conv1(x))\n",
    "        x=nn.functional.relu(self.conv2(x))\n",
    "        x=x.flatten()\n",
    "        x=nn.functional.relu(self.lin(x))\n",
    "        return torch.sigmoid(self.lin2(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier=MODEL()\n",
    "from torch.optim import SGD\n",
    "optimizer=SGD(classifier.parameters(),lr=0.001)\n",
    "lossFUNC=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    tloss=0\n",
    "    for spec,label in DATA:\n",
    "        optimizer.zero_grad()\n",
    "        output=classifier(spec)\n",
    "        loss=lossFUNC(output,label)\n",
    "        tloss+=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch {i} - loss:{tloss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *epoch 0* loss : 46.48808288574219\n",
    "- *epoch 1* loss : 7.791569709777832\n",
    "- *epoch 2* loss : 3.0420384407043457\n",
    "- *epoch 3* loss : 1.5733261108398438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "with torch.no_grad():\n",
    "    for file in posFiles[150:]:\n",
    "        spec=getSpectrogram(loadWav(file))\n",
    "        out=classifier(spec).item()\n",
    "        results.append(round(out)==1)\n",
    "    for file in negFiles[150:]:\n",
    "        spec=getSpectrogram(loadWav(file))\n",
    "        out=classifier(spec).item()\n",
    "        results.append(round(out)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaln=len(posFiles)+len(negFiles)-300\n",
    "correct=sum(results)\n",
    "accuracy=(correct/totaln)*100\n",
    "\n",
    "print(f'test-accuracy : {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*test accuracy* - 95.49%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), 'ears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "identifying chirps in forest recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MODEL()\n",
    "model.load_state_dict(torch.load('ears'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWavfile(path,freq=16e3):\n",
    "    rate,wav=wavfile.read(path)\n",
    "    if wav.ndim>1:\n",
    "        wav=wav[:,0]\n",
    "    nSamples=int((len(wav)/rate)*freq)\n",
    "    nwav=resample(wav,nSamples)\n",
    "    return nwav\n",
    "\n",
    "def getSpectrogramfile(wav):\n",
    "    f,t,Sxx=spectrogram(wav)\n",
    "    Sxx=np.expand_dims(Sxx,axis=0)\n",
    "    mean=Sxx.mean()\n",
    "    std=Sxx.std()\n",
    "    tensor=torch.tensor(list(Sxx),dtype=torch.float32)\n",
    "    tensor=F.normalize(tensor,mean=mean,std=std)\n",
    "    return tensor\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">created wav files using audacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'.\\sounds\\Forest Recordings\\recording_08.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile=loadWavfile(path)\n",
    "spec=getSpectrogramfile(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "ncols=spec.shape[-1]//214\n",
    "with torch.no_grad():\n",
    "    for i in range(ncols):\n",
    "        out=model(spec[:,:,214*i:214*(i+1)])\n",
    "        results.append(out.item())\n",
    "rounds=[round(i) for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expect 24 in recording\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "totalchirps=0\n",
    "for i in groupby(rounds):\n",
    "    totalchirps+=i[0]\n",
    "print(f'expect {totalchirps} in recording') #value=25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6775f008c80a58a24d6fb73f82cc5d490f68df26e158b11540769a2968c169d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
